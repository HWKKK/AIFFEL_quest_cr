{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnQ9awdnOyxIw+ZXCeuA/j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HWKKK/AIFFEL_quest_cr/blob/main/Python/LMS/%5Bhaewon_kim%5DLMS_20250123.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트 (1) load_digits : 손글씨 분류"
      ],
      "metadata": {
        "id": "CCbiiO4m3D-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5Y5b7Qt2VsE",
        "outputId": "c43789f5-3dc3-42fe-e25e-9ac6da9001d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "       pixel_0_0    pixel_0_1    pixel_0_2    pixel_0_3    pixel_0_4  \\\n",
            "count     1797.0  1797.000000  1797.000000  1797.000000  1797.000000   \n",
            "mean         0.0     0.303840     5.204786    11.835838    11.848080   \n",
            "std          0.0     0.907192     4.754826     4.248842     4.287388   \n",
            "min          0.0     0.000000     0.000000     0.000000     0.000000   \n",
            "25%          0.0     0.000000     1.000000    10.000000    10.000000   \n",
            "50%          0.0     0.000000     4.000000    13.000000    13.000000   \n",
            "75%          0.0     0.000000     9.000000    15.000000    15.000000   \n",
            "max          0.0     8.000000    16.000000    16.000000    16.000000   \n",
            "\n",
            "         pixel_0_5    pixel_0_6    pixel_0_7    pixel_1_0    pixel_1_1  ...  \\\n",
            "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000  ...   \n",
            "mean      5.781859     1.362270     0.129661     0.005565     1.993879  ...   \n",
            "std       5.666418     3.325775     1.037383     0.094222     3.196160  ...   \n",
            "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
            "25%       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
            "50%       4.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
            "75%      11.000000     0.000000     0.000000     0.000000     3.000000  ...   \n",
            "max      16.000000    16.000000    15.000000     2.000000    16.000000  ...   \n",
            "\n",
            "         pixel_6_7    pixel_7_0    pixel_7_1    pixel_7_2    pixel_7_3  \\\n",
            "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000   \n",
            "mean      0.206455     0.000556     0.279354     5.557596    12.089037   \n",
            "std       0.984401     0.023590     0.934302     5.103019     4.374694   \n",
            "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
            "25%       0.000000     0.000000     0.000000     1.000000    11.000000   \n",
            "50%       0.000000     0.000000     0.000000     4.000000    13.000000   \n",
            "75%       0.000000     0.000000     0.000000    10.000000    16.000000   \n",
            "max      13.000000     1.000000     9.000000    16.000000    16.000000   \n",
            "\n",
            "         pixel_7_4    pixel_7_5    pixel_7_6    pixel_7_7       target  \n",
            "count  1797.000000  1797.000000  1797.000000  1797.000000  1797.000000  \n",
            "mean     11.809126     6.764051     2.067891     0.364496     4.490818  \n",
            "std       4.933947     5.900623     4.090548     1.860122     2.865304  \n",
            "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
            "25%      10.000000     0.000000     0.000000     0.000000     2.000000  \n",
            "50%      14.000000     6.000000     0.000000     0.000000     4.000000  \n",
            "75%      16.000000    12.000000     2.000000     0.000000     7.000000  \n",
            "max      16.000000    16.000000    16.000000    16.000000     9.000000  \n",
            "\n",
            "[8 rows x 65 columns] \n",
            "\n",
            "\n",
            "decision tree=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96        43\n",
            "           1       0.93      0.80      0.86        35\n",
            "           2       0.94      0.89      0.91        36\n",
            "           3       0.83      0.73      0.78        41\n",
            "           4       0.78      0.84      0.81        38\n",
            "           5       0.71      0.97      0.82        30\n",
            "           6       0.92      0.97      0.95        37\n",
            "           7       0.91      0.78      0.84        37\n",
            "           8       0.76      0.86      0.81        29\n",
            "           9       0.74      0.74      0.74        34\n",
            "\n",
            "    accuracy                           0.85       360\n",
            "   macro avg       0.85      0.85      0.85       360\n",
            "weighted avg       0.86      0.85      0.85       360\n",
            "\n",
            "random forest=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98        43\n",
            "           1       1.00      1.00      1.00        35\n",
            "           2       1.00      1.00      1.00        36\n",
            "           3       1.00      0.98      0.99        41\n",
            "           4       0.95      1.00      0.97        38\n",
            "           5       0.97      1.00      0.98        30\n",
            "           6       1.00      1.00      1.00        37\n",
            "           7       0.97      0.97      0.97        37\n",
            "           8       0.97      0.97      0.97        29\n",
            "           9       0.94      0.94      0.94        34\n",
            "\n",
            "    accuracy                           0.98       360\n",
            "   macro avg       0.98      0.98      0.98       360\n",
            "weighted avg       0.98      0.98      0.98       360\n",
            "\n",
            "SVM=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        43\n",
            "           1       1.00      1.00      1.00        35\n",
            "           2       1.00      1.00      1.00        36\n",
            "           3       1.00      1.00      1.00        41\n",
            "           4       1.00      1.00      1.00        38\n",
            "           5       0.97      1.00      0.98        30\n",
            "           6       1.00      1.00      1.00        37\n",
            "           7       1.00      0.97      0.99        37\n",
            "           8       1.00      0.97      0.98        29\n",
            "           9       0.94      0.97      0.96        34\n",
            "\n",
            "    accuracy                           0.99       360\n",
            "   macro avg       0.99      0.99      0.99       360\n",
            "weighted avg       0.99      0.99      0.99       360\n",
            "\n",
            "SGD=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98        43\n",
            "           1       0.89      0.97      0.93        35\n",
            "           2       1.00      0.97      0.99        36\n",
            "           3       0.97      0.95      0.96        41\n",
            "           4       0.95      1.00      0.97        38\n",
            "           5       0.88      0.93      0.90        30\n",
            "           6       0.97      1.00      0.99        37\n",
            "           7       1.00      0.97      0.99        37\n",
            "           8       1.00      0.76      0.86        29\n",
            "           9       0.87      0.97      0.92        34\n",
            "\n",
            "    accuracy                           0.95       360\n",
            "   macro avg       0.95      0.95      0.95       360\n",
            "weighted avg       0.96      0.95      0.95       360\n",
            "\n",
            "logistic regression=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99        43\n",
            "           1       0.97      0.97      0.97        35\n",
            "           2       1.00      1.00      1.00        36\n",
            "           3       0.95      0.98      0.96        41\n",
            "           4       0.93      1.00      0.96        38\n",
            "           5       0.90      0.93      0.92        30\n",
            "           6       1.00      1.00      1.00        37\n",
            "           7       1.00      0.89      0.94        37\n",
            "           8       0.97      0.97      0.97        29\n",
            "           9       0.97      0.97      0.97        34\n",
            "\n",
            "    accuracy                           0.97       360\n",
            "   macro avg       0.97      0.97      0.97       360\n",
            "weighted avg       0.97      0.97      0.97       360\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "#(2) 데이터 준비\n",
        "data = load_digits()\n",
        "\n",
        "\n",
        "#(3) 데이터 이해하기\n",
        "feature_data = data.data #featrue data 지정하기\n",
        "label_data = data.target #label data 지정하기\n",
        "print(data.target_names) #target names 출력해보기\n",
        "df = pd.DataFrame(feature_data, columns = data.feature_names)\n",
        "df['target'] = data.target\n",
        "print(df.describe(),'\\n\\n') #데이터 describe 해보기\n",
        "\n",
        "\n",
        "#(4) train, test 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_data, label_data, test_size=0.2, random_state = 1) #test_size 및 random_state 임의 지정\n",
        "\n",
        "\n",
        "#(5) 다양한 모델로 학습시켜보기\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "  # decision tree\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train, y_train)\n",
        "decision_tree_y_pred = decision_tree.predict(X_test)\n",
        "print('decision tree=====================================')\n",
        "print(classification_report(y_test, decision_tree_y_pred))\n",
        "\n",
        "  # random forest\n",
        "random_forest = RandomForestClassifier()\n",
        "random_forest.fit(X_train, y_train)\n",
        "random_forest_y_pred = random_forest.predict(X_test)\n",
        "print('random forest=====================================')\n",
        "print(classification_report(y_test, random_forest_y_pred))\n",
        "\n",
        "  # SVM\n",
        "svm_model = svm.SVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_y_pred = svm_model.predict(X_test)\n",
        "print('SVM=====================================')\n",
        "print(classification_report(y_test, svm_y_pred))\n",
        "\n",
        "  # SGD Classifier\n",
        "sgd_model = SGDClassifier()\n",
        "sgd_model.fit(X_train, y_train)\n",
        "sgd_y_pred = sgd_model.predict(X_test)\n",
        "print('SGD=====================================')\n",
        "print(classification_report(y_test, sgd_y_pred))\n",
        "\n",
        "  # logistic regression\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)\n",
        "logistic_y_pred = logistic_model.predict(X_test)\n",
        "print('logistic regression=====================================')\n",
        "print(classification_report(y_test, logistic_y_pred))\n",
        "\n",
        "\n",
        "#(6) 모델을 평가해 보기\n",
        "#선택한 지표: f1 score\n",
        "#선택 이유:\n",
        "# 손글씨 데이터는, 어떤 숫자를 잘 예측했더라도, 다른건 예측하지 못했다면, 전체적인 예측 품질이 떨어지는 데이터셋임.\n",
        "# 따라서 precision과 recall 의 균형을 볼 수 있는 f1 score를 선택함.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------\n",
        "\n",
        "# 프로젝트 (2) load_wine : 와인 분류\n"
      ],
      "metadata": {
        "id": "ikg7rfge3HGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "#(2) 데이터 준비\n",
        "data = load_wine()\n",
        "\n",
        "#(3) 데이터 이해하기\n",
        "feature_data = data.data\n",
        "label_data = data.target\n",
        "print(data.target_names)\n",
        "df = pd.DataFrame(feature_data, columns = data.feature_names)\n",
        "df['target'] = data.target\n",
        "print(df.describe(),'\\n\\n')\n",
        "\n",
        "\n",
        "#(4) train, test 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_data, label_data, test_size=0.2, random_state = 1) #test_size 및 random_state 임의 지정\n",
        "\n",
        "\n",
        "#(5) 다양한 모델로 학습시켜보기\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "  # decision tree\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train, y_train)\n",
        "decision_tree_y_pred = decision_tree.predict(X_test)\n",
        "print('decision tree=====================================')\n",
        "print(classification_report(y_test, decision_tree_y_pred))\n",
        "\n",
        "  # random forest\n",
        "random_forest = RandomForestClassifier()\n",
        "random_forest.fit(X_train, y_train)\n",
        "random_forest_y_pred = random_forest.predict(X_test)\n",
        "print('random forest=====================================')\n",
        "print(classification_report(y_test, random_forest_y_pred))\n",
        "\n",
        "  # SVM\n",
        "svm_model = svm.SVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_y_pred = svm_model.predict(X_test)\n",
        "print('SVM=====================================')\n",
        "print(classification_report(y_test, svm_y_pred))\n",
        "\n",
        "  # SGD Classifier\n",
        "sgd_model = SGDClassifier()\n",
        "sgd_model.fit(X_train, y_train)\n",
        "sgd_y_pred = sgd_model.predict(X_test)\n",
        "print('SGD=====================================')\n",
        "print(classification_report(y_test, sgd_y_pred))\n",
        "\n",
        "  # logistic regression\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)\n",
        "logistic_y_pred = logistic_model.predict(X_test)\n",
        "print('logistic regression=====================================')\n",
        "print(classification_report(y_test, logistic_y_pred))\n",
        "\n",
        "\n",
        "#(6) 모델을 평가해 보기\n",
        "#선택한 지표: f1 score\n",
        "#선택 이유:\n",
        "# 와인 데이터도 손글씨 데이터와 마찬가지로, [0,1,2] 클래스 모두를 맞춰야 의미있음.\n",
        "# 따라서 precision과 recall 의 균형을 볼 수 있는 f1 score를 선택함.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKKg_f4EFqPE",
        "outputId": "a84989b4-e60b-40f1-aa10-2a09cabe7813"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['class_0' 'class_1' 'class_2']\n",
            "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
            "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
            "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
            "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
            "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
            "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
            "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
            "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
            "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
            "\n",
            "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
            "count     178.000000  178.000000            178.000000       178.000000   \n",
            "mean        2.295112    2.029270              0.361854         1.590899   \n",
            "std         0.625851    0.998859              0.124453         0.572359   \n",
            "min         0.980000    0.340000              0.130000         0.410000   \n",
            "25%         1.742500    1.205000              0.270000         1.250000   \n",
            "50%         2.355000    2.135000              0.340000         1.555000   \n",
            "75%         2.800000    2.875000              0.437500         1.950000   \n",
            "max         3.880000    5.080000              0.660000         3.580000   \n",
            "\n",
            "       color_intensity         hue  od280/od315_of_diluted_wines      proline  \\\n",
            "count       178.000000  178.000000                    178.000000   178.000000   \n",
            "mean          5.058090    0.957449                      2.611685   746.893258   \n",
            "std           2.318286    0.228572                      0.709990   314.907474   \n",
            "min           1.280000    0.480000                      1.270000   278.000000   \n",
            "25%           3.220000    0.782500                      1.937500   500.500000   \n",
            "50%           4.690000    0.965000                      2.780000   673.500000   \n",
            "75%           6.200000    1.120000                      3.170000   985.000000   \n",
            "max          13.000000    1.710000                      4.000000  1680.000000   \n",
            "\n",
            "           target  \n",
            "count  178.000000  \n",
            "mean     0.938202  \n",
            "std      0.775035  \n",
            "min      0.000000  \n",
            "25%      0.000000  \n",
            "50%      1.000000  \n",
            "75%      2.000000  \n",
            "max      2.000000  \n",
            "decision tree=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93        14\n",
            "           1       0.75      0.92      0.83        13\n",
            "           2       1.00      0.67      0.80         9\n",
            "\n",
            "    accuracy                           0.86        36\n",
            "   macro avg       0.89      0.84      0.85        36\n",
            "weighted avg       0.88      0.86      0.86        36\n",
            "\n",
            "random forest=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      1.00      0.97        14\n",
            "           1       1.00      0.92      0.96        13\n",
            "           2       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           0.97        36\n",
            "   macro avg       0.98      0.97      0.98        36\n",
            "weighted avg       0.97      0.97      0.97        36\n",
            "\n",
            "SVM=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.79      0.85        14\n",
            "           1       0.58      0.85      0.69        13\n",
            "           2       0.20      0.11      0.14         9\n",
            "\n",
            "    accuracy                           0.64        36\n",
            "   macro avg       0.57      0.58      0.56        36\n",
            "weighted avg       0.62      0.64      0.61        36\n",
            "\n",
            "SGD=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.79      0.88        14\n",
            "           1       0.52      1.00      0.68        13\n",
            "           2       0.00      0.00      0.00         9\n",
            "\n",
            "    accuracy                           0.67        36\n",
            "   macro avg       0.51      0.60      0.52        36\n",
            "weighted avg       0.58      0.67      0.59        36\n",
            "\n",
            "logistic regression=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96        14\n",
            "           1       0.87      1.00      0.93        13\n",
            "           2       1.00      0.89      0.94         9\n",
            "\n",
            "    accuracy                           0.94        36\n",
            "   macro avg       0.96      0.94      0.94        36\n",
            "weighted avg       0.95      0.94      0.95        36\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 프로젝트 (3) load_breast_cancer : 유방암 여부를 진단"
      ],
      "metadata": {
        "id": "Fpn4be2GRyE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "#(2) 데이터 준비\n",
        "data = load_breast_cancer()\n",
        "\n",
        "#(3) 데이터 이해하기\n",
        "feature_data = data.data\n",
        "label_data = data.target\n",
        "print(data.target_names)\n",
        "df = pd.DataFrame(feature_data, columns = data.feature_names)\n",
        "df['target'] = data.target\n",
        "print(df.describe(),'\\n\\n')\n",
        "\n",
        "\n",
        "#(4) train, test 데이터 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature_data, label_data, test_size=0.2, random_state = 1) #test_size 및 random_state 임의 지정\n",
        "\n",
        "\n",
        "#(5) 다양한 모델로 학습시켜보기\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "  # decision tree\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train, y_train)\n",
        "decision_tree_y_pred = decision_tree.predict(X_test)\n",
        "print('decision tree=====================================')\n",
        "print(classification_report(y_test, decision_tree_y_pred))\n",
        "\n",
        "  # random forest\n",
        "random_forest = RandomForestClassifier()\n",
        "random_forest.fit(X_train, y_train)\n",
        "random_forest_y_pred = random_forest.predict(X_test)\n",
        "print('random forest=====================================')\n",
        "print(classification_report(y_test, random_forest_y_pred))\n",
        "\n",
        "  # SVM\n",
        "svm_model = svm.SVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "svm_y_pred = svm_model.predict(X_test)\n",
        "print('SVM=====================================')\n",
        "print(classification_report(y_test, svm_y_pred))\n",
        "\n",
        "  # SGD Classifier\n",
        "sgd_model = SGDClassifier()\n",
        "sgd_model.fit(X_train, y_train)\n",
        "sgd_y_pred = sgd_model.predict(X_test)\n",
        "print('SGD=====================================')\n",
        "print(classification_report(y_test, sgd_y_pred))\n",
        "\n",
        "  # logistic regression\n",
        "logistic_model = LogisticRegression()\n",
        "logistic_model.fit(X_train, y_train)\n",
        "logistic_y_pred = logistic_model.predict(X_test)\n",
        "print('logistic regression=====================================')\n",
        "print(classification_report(y_test, logistic_y_pred))\n",
        "\n",
        "\n",
        "#(6) 모델을 평가해 보기\n",
        "#선택한 지표: recall\n",
        "#선택 이유:\n",
        "# 실제 환자를 환자가 아니라고 판별했을 시 위험할 수 있기 때문\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqw2gnH0Ry_K",
        "outputId": "7ee0d9fd-c41e-4c9a-dd8a-00edcb64db3c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['malignant' 'benign']\n",
            "       mean radius  mean texture  mean perimeter    mean area  \\\n",
            "count   569.000000    569.000000      569.000000   569.000000   \n",
            "mean     14.127292     19.289649       91.969033   654.889104   \n",
            "std       3.524049      4.301036       24.298981   351.914129   \n",
            "min       6.981000      9.710000       43.790000   143.500000   \n",
            "25%      11.700000     16.170000       75.170000   420.300000   \n",
            "50%      13.370000     18.840000       86.240000   551.100000   \n",
            "75%      15.780000     21.800000      104.100000   782.700000   \n",
            "max      28.110000     39.280000      188.500000  2501.000000   \n",
            "\n",
            "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
            "count       569.000000        569.000000      569.000000           569.000000   \n",
            "mean          0.096360          0.104341        0.088799             0.048919   \n",
            "std           0.014064          0.052813        0.079720             0.038803   \n",
            "min           0.052630          0.019380        0.000000             0.000000   \n",
            "25%           0.086370          0.064920        0.029560             0.020310   \n",
            "50%           0.095870          0.092630        0.061540             0.033500   \n",
            "75%           0.105300          0.130400        0.130700             0.074000   \n",
            "max           0.163400          0.345400        0.426800             0.201200   \n",
            "\n",
            "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
            "count     569.000000              569.000000  ...     569.000000   \n",
            "mean        0.181162                0.062798  ...      25.677223   \n",
            "std         0.027414                0.007060  ...       6.146258   \n",
            "min         0.106000                0.049960  ...      12.020000   \n",
            "25%         0.161900                0.057700  ...      21.080000   \n",
            "50%         0.179200                0.061540  ...      25.410000   \n",
            "75%         0.195700                0.066120  ...      29.720000   \n",
            "max         0.304000                0.097440  ...      49.540000   \n",
            "\n",
            "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
            "count       569.000000   569.000000        569.000000         569.000000   \n",
            "mean        107.261213   880.583128          0.132369           0.254265   \n",
            "std          33.602542   569.356993          0.022832           0.157336   \n",
            "min          50.410000   185.200000          0.071170           0.027290   \n",
            "25%          84.110000   515.300000          0.116600           0.147200   \n",
            "50%          97.660000   686.500000          0.131300           0.211900   \n",
            "75%         125.400000  1084.000000          0.146000           0.339100   \n",
            "max         251.200000  4254.000000          0.222600           1.058000   \n",
            "\n",
            "       worst concavity  worst concave points  worst symmetry  \\\n",
            "count       569.000000            569.000000      569.000000   \n",
            "mean          0.272188              0.114606        0.290076   \n",
            "std           0.208624              0.065732        0.061867   \n",
            "min           0.000000              0.000000        0.156500   \n",
            "25%           0.114500              0.064930        0.250400   \n",
            "50%           0.226700              0.099930        0.282200   \n",
            "75%           0.382900              0.161400        0.317900   \n",
            "max           1.252000              0.291000        0.663800   \n",
            "\n",
            "       worst fractal dimension      target  \n",
            "count               569.000000  569.000000  \n",
            "mean                  0.083946    0.627417  \n",
            "std                   0.018061    0.483918  \n",
            "min                   0.055040    0.000000  \n",
            "25%                   0.071460    0.000000  \n",
            "50%                   0.080040    1.000000  \n",
            "75%                   0.092080    1.000000  \n",
            "max                   0.207500    1.000000  \n",
            "\n",
            "[8 rows x 31 columns] \n",
            "\n",
            "\n",
            "decision tree=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.83      0.89        42\n",
            "           1       0.91      0.97      0.94        72\n",
            "\n",
            "    accuracy                           0.92       114\n",
            "   macro avg       0.93      0.90      0.91       114\n",
            "weighted avg       0.92      0.92      0.92       114\n",
            "\n",
            "random forest=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.88      0.93        42\n",
            "           1       0.93      0.99      0.96        72\n",
            "\n",
            "    accuracy                           0.95       114\n",
            "   macro avg       0.95      0.93      0.94       114\n",
            "weighted avg       0.95      0.95      0.95       114\n",
            "\n",
            "SVM=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.74      0.85        42\n",
            "           1       0.87      1.00      0.93        72\n",
            "\n",
            "    accuracy                           0.90       114\n",
            "   macro avg       0.93      0.87      0.89       114\n",
            "weighted avg       0.92      0.90      0.90       114\n",
            "\n",
            "SGD=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      1.00      0.72        42\n",
            "           1       1.00      0.56      0.71        72\n",
            "\n",
            "    accuracy                           0.72       114\n",
            "   macro avg       0.78      0.78      0.72       114\n",
            "weighted avg       0.84      0.72      0.72       114\n",
            "\n",
            "logistic regression=====================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.90      0.93        42\n",
            "           1       0.95      0.97      0.96        72\n",
            "\n",
            "    accuracy                           0.95       114\n",
            "   macro avg       0.95      0.94      0.94       114\n",
            "weighted avg       0.95      0.95      0.95       114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}